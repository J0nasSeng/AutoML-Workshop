{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Architecture Search\n",
    "Neural Architecture Search (NAS) is a special kind of Hyperparameter Optimization (HO) where we aim to tune the model architecture, i.e. structural properties of our model, instead of hyperparameters such as learning rates. Model architectures are hyperparameters as well, however, the search space is combinatorial in size. Thus classical HO-algorithms' runtime scales up very quickly. Specialized algorithms have been developed to search architectures more efficiently, two of them are ENAS and DARTS which we will look at here. Both of these algorithms are *one-shot* approaches to NAS. This means that they don't train each architecture sampled from the search space independently. Both exploit *weight sharing* which simply means that all architectures in the search space share the same model-parameters. This also means that both, the architecture as well as the model-parameters, are being optimized at the same time. This makes learning much faster and EANS and DARTS have been empirically proven to yield state of the art architectures.\n",
    "\n",
    "## The Problem\n",
    "Usually, NAS is defined as a bi-level optimization problem:\n",
    "\\begin{align}\n",
    "    & \\min_{\\mathbf{a} \\in \\mathcal{A}} \\mathcal{L}(\\mathbf{X}_{val}, \\mathbf{y}_{val}; \\mathbf{w}^*) \\\\\n",
    "    \\text{s.t. } & \\mathbf{w}^* = \\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^n} \\mathcal{L}(\\mathbf{X}_{train}, \\mathbf{y}_{train}; \\mathbf{w})\n",
    "\\end{align}\n",
    "We will now consider two different approaches to solve this optimization problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DARTS\n",
    "Differentiable Architecture Search (DARTS) is based on the idea of defining a search-space over architectures using a single DAG. In this DAG, each node is a representation of the input and all edges are computations. Each edge of this DAG is a mixture of different operations, each associated with a parameter indicating how \"important\" a certain operation on some edge is. These parameters are then optimized w.r.t the validation loss via SGD whereas the model-parameters (i.e. parameters of the operations on the edges) are updated w.r.t. training-loss, also using standard backpropagation. \n",
    "\n",
    "DARTS searches for two cell-types: Normal cells and reduction cells. These two cells are exactly the same except that reduction cells reduce the input-dimensions (e.g. down-scaling an image or its feature maps) whereas a normal cell keeps all dimensions the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import datasets\n",
    "from model import CNN\n",
    "from utils import accuracy\n",
    "from nni.retiarii.oneshot.pytorch import DartsTrainer, EnasTrainer\n",
    "from nni.retiarii.nn.pytorch import LayerChoice, InputChoice\n",
    "import ops\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/nni/nas/utils/misc.py:187: RuntimeWarning: ModelNamespace is missing. You might have forgotten to use `@model_wrapper`. Some features might not work. This will be an error in future releases.\n",
      "  warnings.warn('ModelNamespace is missing. You might have forgotten to use `@model_wrapper`. '\n",
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/nni/nas/nn/pytorch/choice.py:252: UserWarning: \"key\" is deprecated. Assuming label.\n",
      "  warnings.warn(f'\"key\" is deprecated. Assuming label.')\n",
      "/home/jonas/anaconda3/envs/automl/lib/python3.9/site-packages/nni/nas/nn/pytorch/choice.py:257: UserWarning: \"reduction\" is deprecated. Ignoring...\n",
      "  warnings.warn(f'\"reduction\" is deprecated. Ignoring...')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/jonas/anaconda3/envs/automl/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCosineAnnealingLR(optim, \u001b[39m10\u001b[39m, eta_min\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m DartsTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m=\u001b[39mcriterion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     unrolled\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m final_architecture \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mexport()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jonas/teaching/nhr/workshops/AutoML-Workshop/notebooks/nas/nas.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinal architecture:\u001b[39m\u001b[39m'\u001b[39m, trainer\u001b[39m.\u001b[39mexport())\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/nni/retiarii/oneshot/pytorch/darts.py:283\u001b[0m, in \u001b[0;36mDartsTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs):\n\u001b[0;32m--> 283\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_one_epoch(i)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/nni/retiarii/oneshot/pytorch/darts.py:177\u001b[0m, in \u001b[0;36mDartsTrainer._train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unrolled_backward(trn_X, trn_y, val_X, val_y)\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward(val_X, val_y)\n\u001b[1;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctrl_optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    180\u001b[0m \u001b[39m# phase 2: child network step\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/nni/retiarii/oneshot/pytorch/darts.py:205\u001b[0m, in \u001b[0;36mDartsTrainer._backward\u001b[0;34m(self, val_X, val_y)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mSimple backward with gradient descent\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m _, loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logits_and_loss(val_X, val_y)\n\u001b[0;32m--> 205\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/automl/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_valid = datasets.get_dataset(\"cifar10\")\n",
    "model = CNN(32, 3, 16, 10, 8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 10, eta_min=0.001)\n",
    "trainer = DartsTrainer(\n",
    "    model=model,\n",
    "    loss=criterion,\n",
    "    metrics=lambda output, target: accuracy(output, target, topk=(1,)),\n",
    "    optimizer=optim,\n",
    "    num_epochs=10,\n",
    "    dataset=dataset_train,\n",
    "    batch_size=64,\n",
    "    log_frequency=10,\n",
    "    unrolled=False\n",
    ")\n",
    "trainer.fit()\n",
    "final_architecture = trainer.export()\n",
    "print('Final architecture:', trainer.export())\n",
    "json.dump(trainer.export(), open('checkpoint.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7613325ebbb1f9384bc508bcd9660c9ee63eceea61e5a048c9a384ab1815eea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
